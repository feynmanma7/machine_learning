{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Expectation Maximization</h1>\n",
    "\n",
    "# 0. Latent Random Variables Model\n",
    "\n",
    "We use multiple random variables to model replationships among high dimensional datasets.\n",
    "\n",
    "+ Independent assumption\n",
    "\n",
    "Such as logistic regression, to model the marginal distribution, \n",
    "\n",
    "> $p(y=1|X; W) = \\sigma(W^T X)$\n",
    "\n",
    "+ Conditional independent assumption\n",
    "\n",
    "Such as naive bayes classifier, to model the joint distribution, \n",
    "\n",
    "> $p(y, X) = p(y) \\prod_{i}p(X_i|y)$\n",
    "\n",
    "+ graphical models\n",
    "\n",
    "To model complex the relationships among random variables, directed or undirected graphical models are used.\n",
    "\n",
    "Model the obeserved random variables directly sometimes are <b>time-consuming</b> and <b>meaningless</b>.\n",
    "\n",
    "Models with latent random variables are introduced.\n",
    "\n",
    "## Single Gaussian Model without latent variables\n",
    "\n",
    "For dataset obeys only one gaussian distribution, the parameters $(\\mu, \\sigma^2)$ are easy to calculate using point estimate method.\n",
    "\n",
    "According to the strong large law of number, \n",
    "\n",
    "> $\\mu = \\mathbb{E}[X] = \\bar{X} = \\frac{1}{N}\\sum_{i=1}^N X_i$\n",
    "\n",
    "> $\\sigma^2 = \\mathbb{E}[(X - \\mu)^2] = \\mathbb{E}[X^2] - \\mathbb{E}^2[X]$\n",
    "\n",
    "> $= \\frac{1}{N} \\sum_{i=1}^N X_i^2 - \\mu^2$\n",
    "\n",
    "## K-Means\n",
    "\n",
    "For dataset contains hidden multiple clusters, the optimized objective  is, \n",
    "\n",
    "> $J = \\sum_{i=1}^N \\sum_{k=1}^K \\mathbb{I}\\left(k = \\arg\\min_k |X_i - C_k|\\right) || X_i - C_k ||^2$\n",
    "\n",
    "> $C_k = \\frac{ \\sum_{i=1}^N \\mathbb{I} (k = \\arg\\min_k |X_i - C_k|) X_i}\n",
    "{\\sum_{i=1}^N \\mathbb{I} (k = \\arg\\min_k |X_i - C_k|)}$ \n",
    "\n",
    "\n",
    "## Gaussian Mixture Model\n",
    "\n",
    "> $p(X) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(X|\\mu_k, \\sigma^2_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Expectation Maximization\n",
    "\n",
    "## 1.0 Maximize Likelihood Estimation (MLE)\n",
    "\n",
    "For models with observed random variable only, maximize likihood estimation is a good method for estimating paramters.\n",
    "\n",
    "For i.i.d dataset $X$, the likelihood function is, \n",
    "\n",
    "> $L(X; \\theta) = \\prod_{i=1}^N p(X_i; \\theta)$\n",
    "\n",
    "For computer and mathematically convenient reason, compute the log-likelihood function, \n",
    "\n",
    "> $l(X; \\theta) = \\log L(X; \\theta) = \\sum_{i=1}^N \\log p(X_i; \\theta)$\n",
    "\n",
    "Thus, \n",
    "\n",
    "> $\\theta^* = \\arg\\max_{\\theta} l(X; \\theta)$\n",
    "\n",
    "Compute the gradient of $l$ with respect to $\\theta$, \n",
    "\n",
    "> $\\frac{\\partial{l}}{\\partial{\\theta}} = \\sum_{i=1}^N \\nabla_{\\theta} \\log p(X_i; \\theta) $\n",
    "\n",
    "It can be seen that the gradient is tractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Latent Random Variable\n",
    "\n",
    "For dataset (X, Z) with observed variable $X$ and hidden variable $Z$, the log-likelihood function of parameters $\\theta$ is, \n",
    "\n",
    "> $l(X; \\theta) = \\sum_{i=1}^N \\log p(X_i; \\theta)$\n",
    "\n",
    "> $= \\sum_{i=1}^N \\log \\sum_{z}^Z p(X_i, z; \\theta)$\n",
    "\n",
    "The gradient of $l(X; \\theta)$ w.r.t $\\theta$ is, \n",
    "\n",
    "> $\\nabla_{\\theta} l(X; \\theta) = \\sum_{i=1}^N \\nabla_{\\theta} \\log \\sum_{z}^Z p(X_i, z; \\theta)$\n",
    "\n",
    "If $z$ has large state spaces, it will be very hard to compute the gradient. Thus, it's not tractable to use maximized likelihood estimate method directly for estimating parameters with latent random variables.\n",
    "\n",
    "## 1.2 Expectation Maximization\n",
    "\n",
    "It's obviously to see that, if the latent variables are <b>observed</b>, the problem will be easy to solve using MLE.\n",
    "\n",
    "### GMM\n",
    "\n",
    "Using GMM as an example, \n",
    "\n",
    "> $p(X; \\theta) = \\sum_{k=1}^K \\pi_k \\mathcal{N} (X|\\mu_k, \\sigma_k^2)$\n",
    "\n",
    "> $\\sum_{k=1}^K \\pi_k = 1, 0 \\le \\pi_k \\le 1$\n",
    "\n",
    "$Z$ is an one-of-$K$ discrete hidden random variable indicates the probability of each cluster.\n",
    "\n",
    "> $p(Z_k = 1) = \\pi_k$\n",
    "\n",
    "> $p(Z) = \\prod_{k=1}^K \\left\\{ \\pi_k \\right\\} ^{Z_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $p(X|Z_k=1;\\theta) = \\mathcal{N}(\\mu_k, \\sigma^2_k)$\n",
    "\n",
    "> $p(X|Z; \\theta) = \\prod_{k=1}^K \\left\\{\\mathcal{N}(\\mu_k, \\sigma^2_k)\\right\\}^{Z_k}$\n",
    "\n",
    "The GMM can also be written as,\n",
    "\n",
    "> $p(X;\\theta) = \\sum_{z} p(X, z; \\theta)$\n",
    "\n",
    "> $= \\sum_z p(z)p(X|z;\\theta)$\n",
    "\n",
    "For every observed data point $x_n$, there is a corresponding one-of-$K$ latent variable $z_{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM\n",
    "\n",
    "#### Jensen's Inequality\n",
    "\n",
    "If $f(x)$ is convex function, then \n",
    "\n",
    "> $\\mathbb{E}[f(x)] \\ge f(\\mathbb{E}[x])$\n",
    "\n",
    "$f(x) = \\log(x)$ is a concave funtion, for $f''(x) = -\\frac{1}{x^2} < 0, \\forall x \\in \\mathbb{R}^{+}$, thus $\\log \\mathbb{E}[x] \\ge \\mathbb{E}[\\log x]$.\n",
    "\n",
    "The <b>equality</b> holds if $f(x)$ is affine or $x$ is constant.\n",
    "\n",
    "Reconsider the log-likelihood function, \n",
    "\n",
    "> $l(X; \\theta) = \\sum_{i=1}^N \\log \\sum_{z} q(z) \\frac{p(X_i, z; \\theta)}{q(z)}$ (Construct an expectation)\n",
    "\n",
    "> $\\ge \\sum_{i=1}^N \\sum_{z} q(z) \n",
    "\\log \\frac{p(X_i, z; \\theta)}{q(z)}$\n",
    "\n",
    "$q(z)$ is a distribution, \n",
    "\n",
    "> $\\sum_{z} q(z) = 1$\n",
    "\n",
    "To hold the equality, the function of $\\log$ is not affine, thus let $\\frac{p(X_i, z; \\theta)}{q(z)}$ be constant.\n",
    "\n",
    "> $q(z) \\propto p(X_i, z; \\theta)$\n",
    "\n",
    "One of the <b>choices</b> of $q(z)$ could be, \n",
    "\n",
    "> $q(z) = \\frac{p(X_i, z; \\theta)}{\\sum_{z} q(z)}$\n",
    "\n",
    "> $= \\frac{p(X_i, z; \\theta)}{\\sum_{z} p(X_i, z; \\theta)}$\n",
    "\n",
    "> $= \\frac{p(X_i, z; \\theta)}{p(X_i; \\theta)}$\n",
    "\n",
    "> $= p(z|X_i; \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $l(X; \\theta) \\ge  \\sum_{i=1}^N \\sum_z p(z|X_i; \\theta) \n",
    "\\log \\frac{p(X_i, z; \\theta)}{p(z|X_i; \\theta)}$\n",
    "\n",
    "> $= \\sum_{i=1}^N \\sum_z p(z|X_i; \\theta) \\log p(X_i, z; \\theta)$\n",
    "\n",
    "> $- p(z|X_i; \\theta) \\log p(z|X_i; \\theta)$ (Constant w.r.t $\\theta$)\n",
    "\n",
    "If we want to maximize the righthand of the inequality formula, we only need to maximzie\n",
    "\n",
    "> $\\sum_z p(z|X_i; \\theta) \\log p(X_i, z; \\theta)$\n",
    "\n",
    "which is the expectation of log-joint distribution $\\log p(X_i, z; \\theta)$ under the distributon of $p(z|X_i; \\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize \n",
    "+ Initialize parameters $\\theta^{old}$.\n",
    "\n",
    "#### E-step\n",
    "\n",
    "+ Compute the expectation of $q(z)$, one of the choices is $p(z|X_i; \\theta^{old})$.\n",
    "\n",
    "#### M-step\n",
    "\n",
    "+ Maximize the Q function of $\\theta$, to get the maximize parameters.\n",
    "\n",
    "> $Q(\\theta, \\theta^{old}) = \\sum_z p(z|X_i; \\theta^{old}) \\log p(X_i, z; \\theta)$\n",
    "\n",
    "> $\\theta^{new} = \\arg\\max_{\\theta} Q(\\theta, \\theta^{old})$\n",
    "\n",
    "#### Check Convergence\n",
    "\n",
    "+ Calculate the log-likelihood, check convergence between iterations.\n",
    "\n",
    "> $l(X; \\theta) = \\sum_{i=1}^N \\log \\left\\{ \\sum_{z} p(z|X_i; \\theta) p(X_i| z; \\theta) \\right \\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
