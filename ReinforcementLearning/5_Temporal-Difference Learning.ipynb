{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Temporal-Diffenrence Learning</h1>\n",
    "\n",
    "# 0. Tabular TD(0) for estimating $v_{\\pi}$\n",
    "\n",
    "+ Initialize $V(s)$ arbitrarily\n",
    "\n",
    "> $V(s) = 0, \\forall s \\in \\mathcal{S}^{+}$\n",
    "\n",
    "+ Repeat (for each episode):\n",
    "\n",
    "> Initialize $S$\n",
    "\n",
    "> Repeat (for each step of episode):\n",
    "\n",
    ">> $A \\leftarrow$ action given by $\\pi$ for $S$\n",
    "\n",
    ">> Take action $A$, observe $R, S'$\n",
    "\n",
    ">> $V(S) \\leftarrow V(S) + \\alpha [R + \\gamma V(S') - V(S)] $\n",
    "\n",
    ">> $S \\leftarrow S'$\n",
    "\n",
    "> until $S$ is terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sarsa (on-policy TD control), output $Q \\approx q_{*}$\n",
    "\n",
    "+ Initialize $Q(s, a), \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}, Q(terminal\\_state, -) = 0$\n",
    "\n",
    "+ Repeat (for each episode):\n",
    "\n",
    "> Initialize $S$\n",
    "\n",
    "> Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\\epsilon-$greedy)\n",
    "\n",
    "> Repeat (for each step of episode):\n",
    "\n",
    ">> Take action $A$, observe $R, S'$\n",
    "\n",
    ">> Choose $A'$ from $S'$ using policy derived from $Q$ (e.g., $\\epsilon-$ greedy)\n",
    "\n",
    ">> $Q(S, A) \\leftarrow Q(S, A) + \\alpha[R + \\gamma Q(S', A') - Q(S, A)]$\n",
    "\n",
    ">> $S \\leftarrow S'; A \\leftarrow A'$\n",
    "\n",
    "> until $S$ is terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Q-learning (off-policy TD control), output $\\pi \\approx \\pi_{*}$\n",
    "\n",
    "+ Initialize $Q(s, a), \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}(s) $, arbitrarily, and $Q(terminal\\_state, -) = 0$\n",
    "\n",
    "+ Repeat (for each episode):\n",
    "\n",
    "> Initialize $S$\n",
    "\n",
    "> Repeat (for each step of episode):\n",
    "\n",
    ">> Choose $A$ from $S$ using policy derived from $Q$ (e.g. $\\epsilon-$greedy)\n",
    "\n",
    ">> Take action $A$, observe $R, S'$\n",
    "\n",
    ">> $Q(S, A) \\leftarrow Q(S, A) + \\alpha[R + \\gamma max_{a} Q(S', a) - Q(S, A)]$\n",
    "\n",
    ">> $S \\leftarrow S'$\n",
    "\n",
    "> until $S$ is terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
