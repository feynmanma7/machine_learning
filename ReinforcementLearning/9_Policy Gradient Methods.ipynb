{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Policy Gradient Methods</h1>\n",
    "\n",
    "# 0. The Policy Gradient Theorem\n",
    "\n",
    "## Parameterized Policy\n",
    "\n",
    "Let $\\theta \\in \\mathbb{R}^{d'}$ be the policy's parameter vector.\n",
    "\n",
    "$J(\\theta)$ is some of the performance measure with respect to the policy parameter.\n",
    "\n",
    "Methods that seek to maximize the performance are called <b>policy  gradient methods</b>, and the update rule of parameter is, \n",
    "\n",
    "> $\\theta_{t+1} = \\theta_{t} + \\alpha \\widehat{\\nabla J(\\theta)}$\n",
    "\n",
    "$\\widehat{\\nabla J(\\theta)}$ is a stochastic estimate whose expectation approximates the gradient of the performance measure with respect to its argument $\\theta_t$.\n",
    "\n",
    "## Performance\n",
    "\n",
    "The performance is defined as the value of the start (some specific but not random node $s_0$) of the episode.\n",
    "\n",
    "> $J(\\theta) = v_{\\pi_{\\theta}}(s_0)$\n",
    "\n",
    "## Policy Gradient Theorem\n",
    "\n",
    "> $\\nabla J(\\theta) \\propto \\sum_{s} \\mu(s) \\sum_{a}q_{\\pi}(s, a)\\nabla_{\\theta} \\pi(a|s, \\theta)$\n",
    "\n",
    "+ Proof\n",
    "\n",
    "> $\\nabla v_{\\pi}(s) = \\nabla \\left[ \\sum_{a} \\pi(a|s) q_{\\pi}(s, a) \\right], \\forall s \\in \\mathcal{S}$\n",
    "\n",
    "> $= \\sum_{a} [\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\nabla q_{\\pi}(s|a)] $ (product rule)\n",
    "\n",
    "> $= \\sum_{a} [\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\nabla \\sum_{s', r} p(s', r|s, a) (r + v_{\\pi}(s'))] $\n",
    "\n",
    "> $= \\sum_{a} [\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\sum_{s'} p(s'|s, a) (\\nabla v_{\\pi}(s'))] $\n",
    "\n",
    "> $= \\sum_{a} [\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\sum_{s'} p(s'|s, a) $\n",
    "\n",
    ">> $\\sum_{a'} [\\nabla \\pi(a'|s') q_{\\pi}(s', a') + \\pi(a'|s') \\sum_{s''} p(s''|s', a') (\\nabla v_{\\pi}(s''))] ]$\n",
    "\n",
    "> $= \\sum_{a} [\\nabla \\pi(a|s) q_{\\pi}(s, a) + \\pi(a|s) \\sum_{s'} p(s'|s, a) $\n",
    "\n",
    ">> $\\sum_{a'} [\\nabla \\pi(a'|s') q_{\\pi}(s', a') + \\pi(a'|s') \\sum_{s''} p(s''|s', a') $\n",
    "\n",
    ">>> $\\sum_{a''} [\\nabla \\pi(a''|s'') q_{\\pi}(s'', a'') + \\pi(a''|s'') \\sum_{s'''} p(s'''|s'', a'') \\nabla v_{\\pi}(s''')]]]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $= \\sum_{a}\\nabla \\pi(a|s)q_{\\pi}(s, a) + $\n",
    "\n",
    ">> $\\sum_{a} \\pi({a|s}) \\sum_{s'}p(s'|s, a)  \\left( \\sum_{a'}\\nabla \\pi(a'|s')q_{\\pi}(s', a') \\right)+  $\n",
    "\n",
    ">>> $\\sum_{a} \\pi({a|s}) \\sum_{s'}p(s'|s, a)  \\sum_{a'} \\pi(a'|s') \\sum_{s''}p(s''|s', a') \\left( \\sum_{a''} \\nabla \\pi(a''|s'')q_{\\pi}(s'', a'') \\right) + $\n",
    "\n",
    ">>>> $...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $=\\sum_{x} Pr(s \\rightarrow x, 0, \\pi) \\sum_{a}\\nabla \\pi(a|s)q_{\\pi}(s, a) + $\n",
    "\n",
    ">> $\\sum_{x} Pr(s \\rightarrow x, 1, \\pi) \\sum_{a}\\nabla \\pi(a|s)q_{\\pi}(s, a) + $\n",
    "\n",
    ">>> $\\sum_{x} Pr(s \\rightarrow x, 2, \\pi) \\sum_{a}\\nabla \\pi(a|s)q_{\\pi}(s, a) + $\n",
    "\n",
    ">>>> $...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $= \\sum_{x \\in \\mathcal{S}} \\sum_{k=0}^{\\infty}Pr(s \\rightarrow x, k, \\pi)\\sum_{a}\\nabla \\pi(a|s)q_{\\pi}(s, a)$\n",
    "\n",
    "$Pr(s \\rightarrow x, k, \\pi)$ is the probability of transitioning from state $s$ to state $x$ in $k$ steps under policy $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, \n",
    "\n",
    "> $\\nabla J(\\theta) = \\nabla v_{\\pi}(s_0)$\n",
    "\n",
    "> $=\\sum_{s}\\left( \\sum_{k=0}^{\\infty} Pr(s_0 \\rightarrow s, k, \\pi) \\right)\n",
    "\\sum_{a} \\nabla \\pi(a|s) q_{\\pi}(s, a)$\n",
    "\n",
    "> $= \\sum_s \\eta(s) \\sum_{a} \\nabla \\pi(a|s) q_{\\pi}(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\eta(s)$ is the expected average time of visiting the state $s$ in a single episode.\n",
    "\n",
    "Let $h(s)$ denote the probability that an episode starts in $s$.\n",
    "\n",
    "Time is spent on the state $s$ if the episode starts in $s$, or transition into $s$ from a preceding state $s'$, \n",
    "\n",
    "> $\\eta(s) = h(s) + \\sum_{s'}\\eta(s') \\sum_{a} \\pi(a|s')p(s|s', a), \\forall s \\in \\mathcal{S}$\n",
    "\n",
    "The fraction of time spent on $s$ is normalized as, \n",
    "\n",
    "> $\\mu(s) = \\frac{\\eta(s)}{\\sum_{s'}\\eta(s')}, \\forall s \\in \\mathcal{S}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\nabla J(\\theta) = \\sum_{s}\\eta(s) \\sum_{a} \\nabla \\pi(a|s) q_{\\pi}(s, a)$\n",
    "\n",
    "> $= \\sum_{s} \\left( \\mu(s) \\sum_{s'}\\eta(s') \\right) \\sum_{a} \\nabla \\pi(a|s) q_{\\pi}(s, a)$\n",
    "\n",
    "> $= \\left( \\sum_{s'}\\eta(s') \\right) \n",
    "\\sum_{s} \\mu(s) \\sum_{a} \\nabla \\pi(a|s) q_{\\pi}(s, a)$\n",
    "\n",
    "> $\\propto \\sum_{s} \\mu(s) \\sum_{a} \\nabla \\pi(a|s) q_{\\pi}(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. REINFORCE, A Monte-Carlo Policy-Gradient Method (episodic)\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "+ Input: a differentiable policy parameterization $\\pi(a|s, \\theta)$\n",
    "\n",
    "+ Initialize policy parameter $\\theta \\in \\mathbb{R}^{d'}$\n",
    "\n",
    "+ Repeat forever:\n",
    "\n",
    "> Generate an episode $S_0, A_0, R_1, ..., S_{T-1}, A_{T-1}, R_T$, following $\\pi(\\centerdot|\\centerdot, \\theta)$\n",
    "\n",
    "> For each step of the episode $t = 0, 1, ..., T-1$:\n",
    "\n",
    ">> $G \\leftarrow$ return from step $t$\n",
    "\n",
    ">> $\\theta \\leftarrow \\theta + \\alpha \\gamma^t G \\nabla_{\\theta} \\ln \\pi(A_t|S_t, \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Induction\n",
    "\n",
    "> $\\nabla J(\\theta) \\propto \\sum_{s} \\mu(s) \\sum_{a} \\nabla \\pi(a|s) q_{\\pi}(s, a)$\n",
    "\n",
    "> $= \\mathbb{E}_{\\pi}\\left[ \\sum_{a} \\nabla \\pi(a|s) q_{\\pi}(s, a) \\right]$\n",
    "\n",
    "> $= \\mathbb{E}_{\\pi}\\left[ \\sum_{a} \\nabla \\pi(a|S_t) q_{\\pi}(S_t, a) \\right]$\n",
    "\n",
    "> $= \\mathbb{E}_{\\pi}\\left[ \\sum_{a} \\pi(a|S_t) \n",
    "\\frac{\\nabla \\pi(a|S_t)}{\\pi(a|S_t) }\n",
    "q_{\\pi}(S_t, a) \\right]$\n",
    "\n",
    "> $= \\mathbb{E}_{\\pi}\\left[ \n",
    "\\frac{\\nabla \\pi(A_t|S_t)}{\\pi(A_t|S_t) }\n",
    "q_{\\pi}(S_t, A_t) \\right]$, (replace $a$ with a sample $A_t \\sim \\pi $)\n",
    "\n",
    "> $= \\mathbb{E}_{\\pi}\\left[ \n",
    "\\frac{\\nabla \\pi(A_t|S_t)}{\\pi(A_t|S_t) }\n",
    "G_t \\right]$, ($\\mathbb{E}_{\\pi}[q_{\\pi}(S_t, A_t)] = G_t$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Update rule\n",
    "\n",
    "> $\\theta_{t+1} = \\theta_t + \\alpha \\frac{\\nabla \\pi(A_t|S_t)}{\\pi(A_t|S_t) }\n",
    "G_t$\n",
    "\n",
    "> $= \\theta_t + \\alpha G_t \\nabla \\ln \\pi(A_t|S_t)$, \n",
    "($\\nabla \\ln x = \\frac{\\nabla x}{x}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
