{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Multi-armed Bandits</h1>\n",
    "\n",
    "# 0. Elements of Reinforcement Learning\n",
    "\n",
    "+ Policy\n",
    "\n",
    "+ Reward Signal\n",
    "\n",
    "+ Value Function\n",
    "\n",
    "+ Model\n",
    "\n",
    "# 1. $k$-armed Bandits\n",
    "\n",
    "The value of action $a$, which is the predicted expectation of rewards, is defined as, \n",
    "\n",
    "> $q_*(a) = \\mathbb{E}[R_t|A_t = a]$\n",
    "\n",
    "$A_t$ is the action selected on time step $t$, and $R_t$ is the corresponding reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e-greedy\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def normal_fn(mu, std):\n",
    "    while True:\n",
    "        yield np.random.normal(mu, std)\n",
    "\n",
    "        \n",
    "def exclude_sample(exclude, low=0, high=9):\n",
    "    rand_n = np.random.randint(low, high)\n",
    "    \n",
    "    if rand_n < exclude:\n",
    "        return rand_n\n",
    "    else:\n",
    "        return exclude + 1\n",
    "        \n",
    "        \n",
    "def bandit_experience(reward_fns, n_step=2000, n_arm=10, epsilon=0.01):\n",
    "\n",
    "    \"\"\"\n",
    "    Q-value\n",
    "        Expected value of each action, \n",
    "        to be learned or counted as average of samples.\n",
    "    \"\"\"\n",
    "    Q_reward = np.zeros(n_arm)\n",
    "    Q_num = np.zeros(n_arm)\n",
    "\n",
    "    total_reward = 0.\n",
    "    avg_rewards = []\n",
    "\n",
    "    for step in range(n_step):\n",
    "        # Choose current action, based on the Q_reward\n",
    "        action = np.argmax(Q_reward)\n",
    "\n",
    "        ## e-greedy\n",
    "        _ = np.random.random()\n",
    "\n",
    "        if _ < epsilon:\n",
    "            action = exclude_sample(action, low=0, high=9)\n",
    "\n",
    "        # Get reward of the action, update the respectively Q_value\n",
    "        reward = next(reward_fns[action])\n",
    "\n",
    "        action_avg_reward = Q_reward[action]\n",
    "        num = Q_num[action]\n",
    "        Q_reward[action] = (action_avg_reward * num + reward) / (num + 1)\n",
    "        Q_num[action] += 1\n",
    "\n",
    "        # avg_reward of each step \n",
    "        total_reward += reward\n",
    "        avg_reward = total_reward / (step+1)\n",
    "        avg_rewards.append(avg_reward)\n",
    "    return np.array(avg_rewards)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward Function\n",
    "reward_fns = []\n",
    "n_arm = 10\n",
    "for i in range(n_arm):\n",
    "    mu = np.random.random()\n",
    "    std = 1\n",
    "    reward_fns.append(normal_fn(mu, std))\n",
    "\n",
    "# Run\n",
    "n_run = 1000\n",
    "n_step = 2000\n",
    "\n",
    "rewards_1 = np.zeros(n_step)\n",
    "rewards_2 = np.zeros(n_step)\n",
    "rewards_3 = np.zeros(n_step)\n",
    "\n",
    "for _ in range(n_run):\n",
    "    rewards_1 += bandit_experience(reward_fns, n_step, n_arm, epsilon=0)\n",
    "    rewards_2 += bandit_experience(reward_fns, n_step, n_arm, epsilon=0.01)\n",
    "    rewards_3 += bandit_experience(reward_fns, n_step, n_arm, epsilon=0.1)\n",
    "\n",
    "rewards_1 /= n_step\n",
    "rewards_2 /= n_step\n",
    "rewards_3 /= n_step\n",
    "\n",
    "# Plot\n",
    "x = np.linspace(1, 2000, 2000)\n",
    "\n",
    "plt.plot(x, rewards_1, 'b-', label='$\\epsilon$=0 greedy')\n",
    "plt.plot(x, rewards_2, 'r-', label='$\\epsilon$=0.01')\n",
    "plt.plot(x, rewards_3, 'g-', label='$\\epsilon$=0.1')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('average reward')\n",
    "plt.title('$\\epsilon$-greedy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
