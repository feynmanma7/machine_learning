{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Finite Markov Decision Processes</h1>\n",
    "\n",
    "# 0. Elements of Finite MDP\n",
    "\n",
    "## Agent\n",
    "\n",
    "Learn ane take <b>action</b> based on the <b>state</b> of the Environment, to obtain the maximum long-term <b>reward</b> come from the environment.\n",
    "\n",
    "## Environment\n",
    "\n",
    "Everything else outside the <b>agent</b>.\n",
    "\n",
    "## Action\n",
    "\n",
    "Move the <b>agent</b> takes to the <b>environment</b> based on the <b>state</b> of the environment.\n",
    "\n",
    "\n",
    "## State\n",
    "\n",
    "Situation the <b>environment</b> is in.\n",
    "\n",
    "## Reward\n",
    "\n",
    "Respond to the <b>agent</b> based on the <b>action</b> of the <b>environment</b>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite MDP\n",
    "\n",
    "The finite MDP is a four-argument function $p: \\mathcal{S} \\times \\mathcal{R} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Maximize the <b>cummulative reward</b> in the long run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Policy $\\pi(a|s)$\n",
    "\n",
    "Based on the current state $s$, the probability of to choose an action $a$.\n",
    "\n",
    "# 2. Value $v_{\\pi}(s)$\n",
    "\n",
    "Expected return when staring in the current state $s$, and following policy $\\pi$ thereafter.\n",
    "\n",
    "> $v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t = s]=\n",
    "\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}|S_t=s\n",
    "\\right], \\forall s \\in \\mathcal{S}$\n",
    "\n",
    "# 3. Q-value $q_{\\pi}(s, a)$\n",
    "\n",
    "Eepected return when starting in the current state $s$, choosing action $a$, and following policy $\\pi$ thereafter.\n",
    "\n",
    "> $q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t|S_t = s, A_t = a]=\n",
    "\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}|S_t=s, A_t=a\\right], \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}\n",
    "$\n",
    "\n",
    "# 4. Bellman equations\n",
    "\n",
    "> $v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t=s]$\n",
    "\n",
    "> $\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_t=s]$\n",
    "\n",
    "> $= \\sum_{a}\\pi(a|s) \\sum_{s'}\\sum_{r}p(s', r|s, a)\\left[r + \n",
    "\\gamma \\mathbb{E}_{\\pi}[G_{t+1}|S_{t+1}=s'] \\right]$\n",
    "\n",
    "> $= \\sum_{a}\\pi(a|s)\\sum_{s'}\\sum_{r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')], \\forall s \\in \\mathcal{S}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Optimal Policies and Optimal Value Functions\n",
    "\n",
    "Policy $\\pi \\ge \\pi^{'}$ if and only if $v_{\\pi}(s) \\ge v_{\\pi^{'}}(s), \\forall s \\in \\mathcal{S}$.\n",
    "\n",
    "<b>Optimal Policy</b>: there is always at least one policy that is better than or equal to all other policies.\n",
    "\n",
    "All of the optimal policies are denoted by $\\pi_{*}$.\n",
    "\n",
    "## Optimal state-value function $v_{*}$\n",
    "\n",
    "> $v_{*}(s) = \\max_{\\pi} v_{\\pi}(s), \\forall s \\in \\mathcal{S}$\n",
    "\n",
    "## Optimal action-value function $q_{*}$\n",
    "\n",
    "> $q_{*}(s, a) = \\max_{\\pi} q_{\\pi}(s, a)$\n",
    "\n",
    "> $q_{*}(s, a) = \\mathbb{E}[R_{t+1} + \\gamma v_{*}(S_{t+1})|S_t=s, A_t=a]$\n",
    "\n",
    "## Bellman optimality equation\n",
    "\n",
    "> $v_{*}(s) = \\max_{a \\in \\mathcal{A}(s)} q_{\\pi_{*}}(s, a)$\n",
    "\n",
    "> $=\\max_{a} \\mathbb{E}_{\\pi_{*}}[G_t|S_t=s, A_t=a]$\n",
    "\n",
    "> $=\\max_{a} \\mathbb{E}_{\\pi_{*}}[R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a]$\n",
    "\n",
    "> $=\\max_{a} \\mathbb{E}[R_{t+1} + \\gamma v_{*}(S_{t+1}) | S_t = s, A_t = a]$\n",
    "\n",
    "> $=\\max_{a} \\sum_{s', r} p(s', r|s, a)[r + \\gamma v_{*}(s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $q_{*}(s, a) = \\mathbb{E}[R_{t+1} + \\gamma \\max_{a'} q_{*}(S_{t+1}, a') | S_t = s, A_t = a]$\n",
    "\n",
    "> $=\\sum_{s', r} p(s', r|s, a)[r + \\gamma \\max_{a'} q_{*}(s', a')]$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
