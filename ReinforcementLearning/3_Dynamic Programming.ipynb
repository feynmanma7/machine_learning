{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dynammic Programming</h1>\n",
    "\n",
    "# 0. Policy Evaluation (Prediction), output $V \\approx V_{\\pi}(s)$\n",
    "\n",
    "## Enviroment\n",
    "The environment is like, \n",
    "\n",
    "> 0, 1, 2, 3\n",
    "\n",
    "> 4, 5, 6, 7\n",
    "\n",
    "> 8, 9, 10, 11\n",
    "\n",
    "> 12, 13, 14, 15\n",
    "\n",
    "## State\n",
    "For state $s \\in \\{0, 1, ..., 15\\}$, with 0 and 15 are terminal states .\n",
    "\n",
    "## Action\n",
    "\n",
    "The agent can go as one of the actions in $\\{up, down, left, right\\}$.\n",
    "\n",
    "## Transition Probability\n",
    "\n",
    "The transition probability of the agent goes from $s$ to $s'$ will be 1 if $s'$ is the next state of $s$ with action in $\\mathcal{S}$ or $s$ is in the side of the matrix, otherwise 0.\n",
    "\n",
    "## Reward\n",
    "\n",
    "If the next state $s'$ is the terminal state, the reward is 0, otherwise -1.\n",
    "\n",
    "## Policy Evaluation\n",
    "\n",
    "+ Initialize\n",
    "\n",
    "> $V(s) = 0, \\forall s \\in \\mathcal{S}^{+}$\n",
    "\n",
    "+ Repeat\n",
    "\n",
    "> $\\Delta \\leftarrow 0$\n",
    "\n",
    "> For each $s \\in \\mathcal{S}$:\n",
    "\n",
    "> > $v \\leftarrow V(s)$\n",
    "\n",
    "> > $V(s) \\leftarrow \\sum_{a}\\pi(a|s) \\sum_{s', r}p(s', r|s, a)[r + \\gamma V(s')]$\n",
    "\n",
    "> > $\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)$\n",
    "\n",
    "+ until $\\Delta < \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# environment\n",
    "env = np.array([[0, 1, 2, 3], \n",
    "                [4, 5, 6, 7], \n",
    "                [8, 9, 10, 11], \n",
    "               [12, 13, 14, 15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# state \n",
    "s = np.array(range(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transition probability\n",
    "n_s = 16\n",
    "n_a = 4 #actions, 0: up, 1: down, 2: left , 3: right\n",
    "p = np.zeros((n_s, n_a, n_s)) # s, a, s'\n",
    "\n",
    "prob = 1. / 4 #prob is equal for all actions\n",
    "\n",
    "#up \n",
    "for s in range(4):\n",
    "    p[s][0][s] = prob\n",
    "for s in range(4, 16):\n",
    "    p[s][0][s-4]= prob\n",
    "    \n",
    "#down\n",
    "for s in range(12, 16):\n",
    "    p[s][1][s] = prob\n",
    "for s in range(0, 12):\n",
    "    p[s][1][s+4] = prob\n",
    "    \n",
    "# left \n",
    "for s in range(0, 16, 4):\n",
    "    p[s][2][s] = prob\n",
    "for s in range(1,16):\n",
    "    if s % 4 != 0:\n",
    "        p[s][2][s-1] = prob\n",
    "\n",
    "# right\n",
    "for s in range(3, 16, 4):\n",
    "    p[s][3][s] = prob\n",
    "for s in range(0, 16):\n",
    "    if s % 4 != 3:\n",
    "        p[s][3][s+1] = prob\n",
    "        \n",
    "# terminal state\n",
    "for a in range(4):\n",
    "    for s1 in range(16):\n",
    "        p[0][a][s1] = 0\n",
    "        p[15][a][s1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Order for args: \n",
    "# s, a, s1, gamma, p, pi, V \n",
    "\n",
    "# reward function\n",
    "def reward_func(s, a, s1):\n",
    "    if s1 == 0 or s1 == 15:\n",
    "        return 0\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expect_value(s, gamma, p, V):\n",
    "    \"\"\"\n",
    "    s: current state\n",
    "    gamma: discount factor\n",
    "    V: value function\n",
    "    p: transition probability, s, a, s1\n",
    "    \"\"\"\n",
    "    \n",
    "    # pi(a|s) = 1 / n_a, for all actions\n",
    "    \n",
    "    e_v = 0\n",
    "    for s1 in range(16):\n",
    "        for a in range(4):\n",
    "            #pi_s_a = 1. / 4\n",
    "            p_s_a_s1 = p[s][a][s1]\n",
    "            r = reward_func(s, a, s1)\n",
    "            e_v += p_s_a_s1 * (r + gamma * V[s1])\n",
    "\n",
    "    return e_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_value(gamma, p, V):\n",
    "    delta = 0\n",
    "    for s in range(16):\n",
    "        v = V[s]\n",
    "        V[s] = expect_value(s, gamma, p, V)\n",
    "        delta = max(delta, abs(v - V[s]))\n",
    "    return V, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "V = np.zeros(16)\n",
    "gamma = 1.0\n",
    "\n",
    "def policy_evaluation(gamma, p, V):\n",
    "    delta = 1e10\n",
    "    epsilon = 1e-4\n",
    "    while delta > epsilon:\n",
    "        V, delta = evaluate_value(gamma, p, V)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = policy_evaluation(gamma, p, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , -12.99934883, -18.99906386, -20.9989696 ,\n",
       "       -12.99934883, -16.99920093, -18.99913239, -18.99914232,\n",
       "       -18.99906386, -18.99913239, -16.9992679 , -12.9994534 ,\n",
       "       -20.9989696 , -18.99914232, -12.9994534 ,   0.        ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Policy Iteration, output $V \\approx v_{*}$ and $\\pi \\approx \\pi_{*}$\n",
    "\n",
    "+ Initialization\n",
    "\n",
    "> $V(s) \\in \\mathbb{R}, \\pi(s) \\in \\mathcal{A}(s)$ arbitrarily\n",
    "\n",
    "+ Policy Evaluation\n",
    "\n",
    "+ Policy Improvement\n",
    "\n",
    "> $policy\\_stable \\leftarrow true$\n",
    "\n",
    "> For each $s \\in \\mathcal{S}$:\n",
    "\n",
    "> > $old\\_action \\leftarrow \\pi(s)$\n",
    "\n",
    "> > $\\pi(s) \\leftarrow \\arg\\max_{a}\\sum_{s', r}p(s', r|s, a)[r + \\gamma V(s')]$\n",
    "\n",
    "> > If $old\\_action \\ne \\pi(s)$, then $policy\\_stable \\leftarrow false$\n",
    "\n",
    "> If $policy\\_stable = true$, then stop, else goto 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expect_pi(s, gamma, p, V):\n",
    "    max_a = -1\n",
    "    max_r = -1e10\n",
    "    \n",
    "    for a in range(4):\n",
    "        e_v = 0 # expect_value\n",
    "        for s1 in range(16):\n",
    "            p_s_a_s1 = p[s][a][s1]\n",
    "            r = reward_func(s, a, s1)\n",
    "            e_v += p_s_a_s1 * (r + gamma * V[s1])\n",
    "        \n",
    "        if e_v > max_r:\n",
    "            max_r = e_v\n",
    "            max_a = a\n",
    "    \n",
    "    return max_a\n",
    "\n",
    "\n",
    "def policy_improvement(gamma, p, pi, V):\n",
    "    policy_stable = True\n",
    "    \n",
    "    for s in range(16):\n",
    "        old_a = pi[s]\n",
    "\n",
    "        pi[s] = expect_pi(s, gamma, p, V)\n",
    "\n",
    "        if old_a != pi[s]:\n",
    "            policy_stable = False\n",
    "\n",
    "    return V, pi, policy_stable        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.         -12.99940341 -18.99914232 -20.99905596 -12.99940341\n",
      " -16.9992679  -18.99920511 -18.9992142  -18.99914232 -18.99920511\n",
      " -16.99932926 -12.99949921 -20.99905596 -18.9992142  -12.99949921\n",
      "   0.        ]\n",
      "[0. 2. 2. 2. 0. 0. 2. 1. 0. 0. 3. 1. 0. 3. 3. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "n_s = 16\n",
    "n_a = 4\n",
    "V = np.zeros(n_s)\n",
    "pi = np.zeros(n_s) \n",
    "\n",
    "policy_stable = False\n",
    "\n",
    "# policy iteration\n",
    "while policy_stable == False:\n",
    "    V = policy_evaluation(gamma, p, V)\n",
    "    V, pi, policy_stable = policy_improvement(gamma, p, pi, V)\n",
    "\n",
    "\n",
    "print(V)\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
