{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>XGBoost</h1>\n",
    "\n",
    "# 0. Predict Value\n",
    "\n",
    "Use Additive Model, in the $t$-th iteration,\n",
    "\n",
    "> $\\hat{y^{(t)}} = \\phi(x) = \\sum_{i=1}^{t}  f^{(i)}(x), f^{(i)}(x) \\in \\mathcal{F}$\n",
    "\n",
    "$\\mathcal{F} = \\{f(x) = w_{q(x)}\\}(q: \\mathbb{R}^m \\rightarrow T, w \\in \\mathbb{R}^T)$\n",
    "\n",
    "$m$ is the input dimension, $T$ is the number of leaf nodes for current tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loss Function\n",
    "\n",
    "## Total Loss\n",
    "\n",
    "> $L(\\phi) = \\sum_{i=1}^n L(y_i, \\hat{y^{t}_i}) + \n",
    "\\sum_{j=1}^t \\Omega(f^{(j)})$\n",
    "\n",
    "> $\\Omega(f^{(j)}) = \\gamma T^{(j)} + \n",
    "\\frac{1}{2} \\lambda \\sum_{k=1}^{T^{(j)}} ||w_k||^2$\n",
    "\n",
    "## Greedy Solution\n",
    "\n",
    "For additive model, problem is solved iteratively in greedy way.\n",
    "\n",
    "According to the Taylor's expansion, \n",
    "\n",
    "> $L^{(t)} = \\sum_{i=1}^n L(y_i, \\hat{y^{(t)}_i}) + \\Omega(f^{(t)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\approx \\sum_{i=1}^n [L(y_i, \\hat{y^{(t-1)}})\n",
    "+ g_i f^{(t)}(x_i) \n",
    "+ \\frac{1}{2} H_i {f^{(t)}}^2(x_i)] + \n",
    "\\gamma T + \\frac{1}{2} \\lambda \\sum_{k=1}^{T} ||w_k||^2$\n",
    "\n",
    "where $g_i$ and $H_i$ are the first and second order of derivatives of loss function with respect to predict value of last iteration,\n",
    "\n",
    "> $g_i = \\frac{\\partial{L(y_i, \\hat{y^{(t-1)}_i})}}{\\partial{\\hat{y^{(t-1)}_i}}}$\n",
    "\n",
    "> $H_i = \\frac{\\partial{g_i}}\n",
    "{\\partial{\\hat{y^{(t-1)}_i}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex property\n",
    "\n",
    "The basic loss function and the regularization function are both convex, thus we can obtain the minimum value of the completely loss function of $L^{(t)}$ in the point where the first-order derivative value is 0.\n",
    "\n",
    "> $L^{(t)} = \\sum_{i=1}^n [L(y_i, \\hat{y_i^{(t-1)}})] \n",
    "+ \\sum_{i=1}^n [g_i f^{(t)} (x_i) + \\frac{1}{2} H_i {f^{(t)}}^2 (x_i)]\n",
    "+ \\gamma T + \\frac{1}{2} \\lambda \\sum_{k=1}^T ||w_k||^2$\n",
    "\n",
    "In the $t$-th iteration, $\\sum_{i=1}^n [L(y_i, \\hat{y_i^{(t-1)}})] $ is a const with respect the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance set of Set (Important)\n",
    "Define $I_j = \\{i|q(x_i) = j\\}$ is the instance set of leaf $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orignal loss function becomes, \n",
    "\n",
    "> $L^{(t)} = const + \\sum_{j=1}^T [ (\\sum_{i \\in I_j } g_i) w_j\n",
    "+ \\frac{1}{2}((\\sum_{i \\in I_j} H_i) + \\lambda) w_j^2]\n",
    "+ \\gamma T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal value\n",
    "\n",
    "For a fixed structure $q(x)$, the optimal value of $w_j$ is, \n",
    "\n",
    "> $w^{*}_j = - \\frac{\\sum_{i \\in I_j} g_i}\n",
    "{(\\sum_{i \\in I_j} H_j) + \\lambda}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Split Way\n",
    "\n",
    "Substitute $w^*_j$ into $L^{(t)}$, \n",
    "\n",
    "> $L^{(t)} = const - \\frac{1}{2} \\sum_{j=1}^T [\n",
    "\\frac{ (\\sum_{i \\in I_j} g_i)^2 }\n",
    "{ (\\sum_{i \\in I_j} H_i) + \\lambda }] + \\gamma T$\n",
    "\n",
    "Assume for a chosen split attribute and split value, the instance set of leaf $I$ is split into two separated sets $I_L$ and $I_R$.\n",
    "\n",
    "We will choose a best split attribute and value that <b>decreases</b> the loss function <b>most</b> after split.\n",
    "\n",
    "> $split = \\arg \\max_{ \\{ I, I_L, I_R \\} } L_{split}$\n",
    "\n",
    "> $= L_{before\\_split} - L_{after\\_split}$\n",
    "\n",
    "> $= \\frac{1}{2} \\left[ \n",
    "\\frac{( \\sum_{i \\in I_L} g_i )^2} { \\sum_{i \\in I_L} H_i + \\lambda }\n",
    "+ \\frac{( \\sum_{i \\in I_R} g_i )^2} { \\sum_{i \\in I_R} H_i + \\lambda }\n",
    "- \\frac{( \\sum_{i \\in I} g_i )^2} { \\sum_{i \\in I} H_i + \\lambda }\n",
    "\\right] - \\gamma $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
