{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Linear Models for Regression</h1>\n",
    "\n",
    "Reference:\n",
    "\n",
    "[1] PRML.\n",
    "[2] MLAPP.\n",
    "\n",
    "# 0. Linear Basis Function Models\n",
    "\n",
    "## Linear Regression\n",
    "> $y(x, w) = w_0 + w_1 \\phi(x_1) + ... w_M \\phi(x_M)$\n",
    "\n",
    "> $= \\sum_{i=1}^M w_i \\phi(x_i) = W^{T}\\phi(x)$\n",
    "\n",
    "## Maximum Likelihood\n",
    "\n",
    "Assume the target variable $t$ is the summation of <b>deteministic</b> function $y(x, w)$ and gaussian noise $\\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})$.\n",
    "\n",
    "> $t = y(x, w) + \\epsilon$\n",
    "\n",
    "Thus, $\\mathbb{E}[t|x, w] = y(x, w)$, $Var[t|x, w] = \\beta^{-1}$,\n",
    "\n",
    "> $p(t|x, w) = \\mathcal{N}(t|y(x, w), \\beta^{-1})$\n",
    "\n",
    "For $N$ independent identically distributed samples $\\{X_1, X_2, ..., X_N\\}$, the likelihood function is,\n",
    "\n",
    "> $L(X, w) = \\prod_{i=1}^N p(t_i|X_i, w)$\n",
    "\n",
    "Compute the log-likelihood, (to avoid the underflow problem), \n",
    "\n",
    "> $l(X, w) = \\log L(X, w) = \\sum_{i=1}^N \\log p(t_i|X_i, w)$\n",
    "\n",
    "The log-likelihood function $l(X, w)$ is <b>concave</b> of $w_j$, \n",
    "thus compute the derivate of $l$ with respect to $w_j$, let it be zero, \n",
    "we will obtain the optimal value of parameters $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bias Variance Trade-off\n",
    "\n",
    "> $\\mathbb{E}[(y - \\hat{y})^2] = \\mathbb{E}[y^2 + {\\hat{y}}^2 - 2y\\hat{y}]$\n",
    "\n",
    "> $= \\mathbb{E}[y^2] + \\mathbb{E}[{\\hat{y}}^2] - 2\\mathbb{E}[y]\\mathbb{E}[\\hat{y}]$\n",
    "\n",
    "> $= Var[y] + \\mathbb{E}^2[y] + Var[\\hat{y}] + \\mathbb{E}^2[\\hat{y}] - 2 y \\mathbb{E}[\\hat{y}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $= Var[y] + Var[\\hat{y}] + \\{\\mathbb{E}^2[\\hat{y}] - 2 y \\mathbb{E}[\\hat{y}]+ \\mathbb{E}^2[y] \\}$\n",
    "\n",
    "> $= \n",
    "\\underbrace{Var[y]}_{noise} + \n",
    "\\underbrace{Var[\\hat{y}]}_{variance} + \n",
    "\\underbrace{\\{ y - \\mathbb{E}[\\hat{y}] \\}^2}_{bias^2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bayesian Linear Regression\n",
    "\n",
    "The likelihood of a linear regression problem is, \n",
    "\n",
    "> $L(X, w) = \\prod_{i=1}^N p(t_i|X_i, w)$\n",
    "\n",
    "where $p(t|x, w) = \\mathcal{N}(t|y(x, w), \\beta^{-1})$, \n",
    "\n",
    "thus the conjugate prior of the likelihood is also a Gaussian distribution.\n",
    "\n",
    "> $p(w) = \\mathcal{N}(w|\\mu_0, S_0)$\n",
    "\n",
    "The posterior distribution of $w$ is, \n",
    "\n",
    "> $p(w|t) = \\mathcal{N}(w|\\mu_1, S_1)$\n",
    "\n",
    "> $\\mu_1 = S_1(S_0^{-1}m_0 + \\beta \\phi^T t)$\n",
    "\n",
    "> $S_1^{-1} = S_0^{-1} + \\beta \\phi^T \\phi$\n",
    "\n",
    "The predictive result will be, \n",
    "\n",
    "> $p(t^{test}|t^{train}, w) = \\int p(t^{test}|x, w)p(w|t^{train})dw$\n",
    "\n",
    "Almost everything about Gaussian Distribution is analytical.\n",
    "\n",
    "> $p(t^{test}|t^{train}, w) = \\mathbb{E}_{w}[p(t^{test}|x, w)]$ \n",
    "\n",
    "To compute the expectation of the likihood function, if it is not analytical, we will use some approximation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sampling Methods\n",
    "\n",
    "## 3.0 Law of Large Number (Core)\n",
    "\n",
    "According to the law of large number, \n",
    "\n",
    "if $f(x_i)$ is drawn from the distribution of $p$.\n",
    "\n",
    "> $\\mathbb{E}_{p}[f(x)] = \\int f(x) p(x) dx \\thickapprox \\frac{1}{N} \\sum_{i=1}^N f(x_i)$\n",
    "\n",
    "## 3.1 Standard Distribution\n",
    "\n",
    "> <b>Theorem</b>: Assume we have a uniform random number generator $z \\sim U(0, 1)$, \n",
    "> $F^{-1}(z) \\sim F$.\n",
    "\n",
    "Proof:\n",
    "\n",
    "> $p(F^{-1}(z) \\le x)$\n",
    "\n",
    "> $= p(z \\le F(x))$, (Apply $F$ to both sides)\n",
    "\n",
    "> $= F(x)$, (Because $p(z \\le y) = \\int_{0}^{y}dz = y$)\n",
    "\n",
    "If we want to draw samples from $p(x)$ which is hard to draw from, we can \n",
    "\n",
    "> + compute the cdf (cummulative distribution function $F(x)$;\n",
    "> + compute the inverse of cdf $F^{-1}(x)$;\n",
    "> + sample from the uniform distribution $z \\sim U(0, 1)$;\n",
    "> + obtain the sample value from $p(x)$, which is $F^{-1}(u)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: \n",
    "\n",
    "> $p(x=1) = 0.2, p(x=2) = 0.3, p(x=3) = 0.1, p(x=4) = 0.4$\n",
    "\n",
    "> $F(x=1) = p(x \\le 1) = 0.2, F(x=2) = p(x \\le 2) = 0.5, F(x=3) = p(x \\le 3) = 0.6, F(x=4)  = p(x \\le 4) = 1.0$\n",
    "\n",
    "Let $y = F^{-1}(x)$, $f(y) = x$,\n",
    "\n",
    "> $f(y=0.2) = 1, f(y=0.5) = 2, f(y=0.6) = 3, f(y=1.0) = 4$\n",
    "\n",
    "Draw $z \\sim U(0, 1)$, insert into $F^{-1}(z)$ we will obtain the samples drawn from $p(x)$,\n",
    "\n",
    "> $x = 1, if 0 < z \\le 0.2$\n",
    "\n",
    "> $x = 2, if 0.2 < z \\le 0.5$\n",
    "\n",
    "> $x = 3, if 0.5 < z \\le 0.6$\n",
    "\n",
    "> $x = 4, if 0.6 < z < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Rejection Methods\n",
    "\n",
    "Usually the cdf of $p(x)$ is hard to compute.\n",
    "\n",
    "Use a proposal distribution $q(x)$ which is easy to draw from, and a constant $M$, \n",
    "\n",
    "> $Mq(x) \\ge p(x)$\n",
    "\n",
    "<b>Sampling process</b>\n",
    "\n",
    "+ Sample $x$ from $q(x)$\n",
    "\n",
    "+ Sample from the uniform distribution $u \\sim U(0, 1)$\n",
    "\n",
    "+ Check if $u \\le \\frac{p(x)}{Mq(x)}$, keep $x$, else reject the current sample.\n",
    "\n",
    "Sometimes, an unnormalized distribution $\\widetilde{p}(x)$ of $p(x)$ is used.\n",
    "\n",
    "<b>Cons</b>\n",
    "\n",
    "The sampling efficiency is very low cause of rejection of some samples.\n",
    "\n",
    "## 3.3 Importance Sampling\n",
    "\n",
    "To overcome the inefficiency of rejection sampling methods, we keep all of the samples drawn, but with different weights. \n",
    "\n",
    "Actually, the samples should be discarded in the rejection methods should have a low weight.\n",
    "\n",
    "Also use a proposal distribution which is easy to draw from.\n",
    "\n",
    "When to estimate the expectation of $f(x)$ under the distribution of $p(x)$, \n",
    "\n",
    "> $\\mathbb{E}_{p(x)}[f(x)] = \\int f(x) p(x) dx$\n",
    "\n",
    "The accuracy not only relies on samples of large probabity, but also with <b>large absolute value</b> $|f(x)|$ which contribute a lot.\n",
    "\n",
    "> $\\mathbb{E}_{p(x)}[f(x)] = \\int f(x) p(x) dx$\n",
    "\n",
    "> $= \\int f(x) \\frac{p(x)}{q(x)} q(x) dx$\n",
    "\n",
    "> $\\thickapprox \\frac{1}{N} \\sum_{i=1}^N f(x_i)w_i$\n",
    "\n",
    "where $w_i = \\frac{p(x_i)}{q(x_i)}$.\n",
    "\n",
    "<b>Sampling process</b>\n",
    "\n",
    "+ Sample $x \\sim q(x)$\n",
    "\n",
    "+ Compute the weight $\\frac{p(x)}{q(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.4 Markov Chain Monte Carlo\n",
    "\n",
    "The rejection methods and importance sampling may have severe limitations especially in <b>high dimensions</b>.\n",
    "\n",
    "The MCMC methods are introduced.\n",
    "\n",
    "+ A proposal distribution $q(x)$ which is easy to draw from, and with the <b>markov property</b> that the current state relies and only relies on the past state\n",
    "\n",
    "+ Unnomalized version the distribution $p(x) = \\frac{\\widetilde{p}(x)}{Z_p}$, where $\\widetilde{p}(x)$ is easy to evaluate\n",
    "\n",
    "To borrow the idea from rejection methods, we need to set up some conditions to keep or to reject the samples drawn from $q(x)$.\n",
    "\n",
    "Unlike find a consant M to make $Mq(x)$ envelope $p(x)$, here we concentrate on the proposal distributions.\n",
    "\n",
    "\n",
    "### 3.4.0 Markov Chain\n",
    "\n",
    "#### Markov Property\n",
    "\n",
    "> $p(z^{(t+1)} | z^{(1)}, z^{(2)}, ..., z^{(t)}) = p(z^{(t+1)}|z^{(t)})$\n",
    "\n",
    "#### Transition\n",
    "\n",
    "Define the transition probability from state $a$ to state $b$ is\n",
    "\n",
    "> $T(a, b) = p(b|a)$,\n",
    "\n",
    "#### Invariant Distribution\n",
    "\n",
    "A distribution is said to be <b>invariant</b> or <b>stationary</b> with respect to a Markov Chain if each step in the chain leaves that distribution invariant.\n",
    "\n",
    "For a homogenous Markov Chain, with Transition probability $T(a, b)$, the distribution $p(x)$ is invariant if, \n",
    "\n",
    "> $p(b) = \\sum_{a} T(a, b)p(a)$\n",
    "\n",
    "<b>Note:</b> There may have more than one invariant distribution (e.g. the identity distribution).\n",
    "\n",
    "#### Detailed Balance\n",
    "\n",
    "> $p(a)T(a, b) = p(b)T(b, a)$\n",
    "\n",
    "<b>Theroem: </b> A transition probability that satifies the detailed balance property, it will lead to the distribution invariant.\n",
    "\n",
    "<b>Proof:</b>\n",
    "\n",
    "> $\\sum_{a} p(a)T(a, b) = \\sum_{a} p(b)T(b, a) = p(b) \\sum_{a} T(b, a) = p(b) \\sum_{a} p(a|b) = p(b)$\n",
    "\n",
    "### 3.4.1 Metropolis Hastings Algorithms\n",
    "\n",
    "#### 3.4.1.0 Symmetric Proposal, Metropolis Algorithms\n",
    "\n",
    "Assume that $q(x)$ is <b>symmetric</b>, \n",
    "\n",
    "> $q(z_a | z_b) = q(z_b | z_a)$\n",
    "\n",
    "<b>Sampling Process</b>\n",
    "\n",
    "+ Draw $u \\sim U(0, 1)$\n",
    "\n",
    "+ Draw $z \\sim q(z)$\n",
    "\n",
    "+ Compute the accept probability $A(z|z^{(t)}) = \\min \\left(1, \\frac{\\widetilde{p}(z)}{\\widetilde{p}(z^{(t)})} \\right)$\n",
    "\n",
    "+ If $u \\ge A(z|z^{(t)})$, accept the current sample, $z^{(t+1)} = z$, else use the previous sample $z^{(t+1)} = z^{(t)}$\n",
    "\n",
    "<b>Proof:</b> Set up a Markov Chain with the detailed balance condition, we will obtain the invariant distribution as desired.\n",
    "\n",
    "> $p(z^{(t)}) A(z|z^{(t)}) = p(z^{(t)}) \\min \\left(1, \\frac{\\widetilde{p}(z)}{\\widetilde{p}(z^{(t)})} \\right) $\n",
    "\n",
    "> $= p(z^{(t)}) \\min \\left(1, \\frac{p(z)}{p(z^{(t)})} \\right)$\n",
    "\n",
    "> $= \\min(p(z^{(t)}), p(z))$\n",
    "\n",
    "> $= p(z) \\min(\\frac{p(z^{(t)})}{p(z)}, 1)$\n",
    "\n",
    "> $= p(z) \\min(\\frac{ \\widetilde{p}(z^{(t)}) }{\\widetilde{p}(z)}, 1)$\n",
    "\n",
    "> $= p(z)A(z^{(t)}|z)$\n",
    "\n",
    "The detailed balance is obtained.\n",
    "\n",
    "#### 3.4.1.1 Asymmetric\n",
    "\n",
    "The accept probability becomes, \n",
    "\n",
    "> $A(z|z^{(t)}) = \\min \\left(1, \n",
    "\\frac{\\widetilde{p}(z) q(z^{(t)}|z) }\n",
    "{\\widetilde{p}(z^{(t)}) q(z|z^{(t)}))} \\right)$\n",
    "\n",
    "<b>Proof:</b>\n",
    "\n",
    "> $p(z_a) q(z_b|z_a) A(z_b|z_a) = p(z_a) q(z_b|z_a) \n",
    "\\min \\left(1, \\frac{ \\widetilde{p}(z_a)  q(z_b|z_a)}{ \\widetilde{p}(z_b) q(z_a|z_b)}\n",
    "\\right)$\n",
    "\n",
    "> $= p(z_a) q(z_b|z_a) \n",
    "\\min \\left(1, \\frac{ p(z_a)  q(z_b|z_a)}{ p(z_b) q(z_a|z_b)}\n",
    "\\right)$\n",
    "\n",
    "> $= p(z_a)q(z_b|z_a) \\min \\left( \\frac{p(z_b)q(z_a|z_b)}{p(z_a)q(z_b|z_a)}, 1  \\right)$\n",
    "\n",
    "> $= \\min \\left( p(z_b)q(z_a|z_b), p(z_a)p(z_b|z_a) \\right)$\n",
    "\n",
    "> $= p(z_b)q(z_a|z_b) \\min \\left(1, \\frac{p(z_a)p(z_b|z_a)}{p(z_b)q(z_a|z_b)} \\right)$\n",
    "\n",
    "Given a proposal distrbution $q(z)$ and an accept rate $A(z_b|z_a)$, view the product of them as the <b>transition</b> probability function,\n",
    "we obtain a Markove Chain with its <b>detailed balance</b> property satisfied, which means we can obtain the invariant distribution $p(z)$ as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
