{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Linear Models for Regression</h1>\n",
    "\n",
    "Reference:\n",
    "\n",
    "[1] PRML.\n",
    "[2] MLAPP.\n",
    "\n",
    "# 0. Linear Basis Function Models\n",
    "\n",
    "## Linear Regression\n",
    "> $y(x, w) = w_0 + w_1 \\phi(x_1) + ... w_M \\phi(x_M)$\n",
    "\n",
    "> $= \\sum_{i=1}^M w_i \\phi(x_i) = W^{T}\\phi(x)$\n",
    "\n",
    "## Maximum Likelihood\n",
    "\n",
    "Assume the target variable $t$ is the summation of <b>deteministic</b> function $y(x, w)$ and gaussian noise $\\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})$.\n",
    "\n",
    "> $t = y(x, w) + \\epsilon$\n",
    "\n",
    "Thus, $\\mathbb{E}[t|x, w] = y(x, w)$, $Var[t|x, w] = \\beta^{-1}$,\n",
    "\n",
    "> $p(t|x, w) = \\mathcal{N}(t|y(x, w), \\beta^{-1})$\n",
    "\n",
    "For $N$ independent identically distributed samples $\\{X_1, X_2, ..., X_N\\}$, the likelihood function is,\n",
    "\n",
    "> $L(X, w) = \\prod_{i=1}^N p(t_i|X_i, w)$\n",
    "\n",
    "Compute the log-likelihood, (to avoid the underflow problem), \n",
    "\n",
    "> $l(X, w) = \\log L(X, w) = \\sum_{i=1}^N \\log p(t_i|X_i, w)$\n",
    "\n",
    "The log-likelihood function $l(X, w)$ is <b>concave</b> of $w_j$, \n",
    "thus compute the derivate of $l$ with respect to $w_j$, let it be zero, \n",
    "we will obtain the optimal value of parameters $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bias Variance Trade-off\n",
    "\n",
    "> $\\mathbb{E}[(y - \\hat{y})^2] = \\mathbb{E}[y^2 + {\\hat{y}}^2 - 2y\\hat{y}]$\n",
    "\n",
    "> $= \\mathbb{E}[y^2] + \\mathbb{E}[{\\hat{y}}^2] - 2\\mathbb{E}[y]\\mathbb{E}[\\hat{y}]$\n",
    "\n",
    "> $= Var[y] + \\mathbb{E}^2[y] + Var[\\hat{y}] + \\mathbb{E}^2[\\hat{y}] - 2 y \\mathbb{E}[\\hat{y}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $= Var[y] + Var[\\hat{y}] + \\{\\mathbb{E}^2[\\hat{y}] - 2 y \\mathbb{E}[\\hat{y}]+ \\mathbb{E}^2[y] \\}$\n",
    "\n",
    "> $= \n",
    "\\underbrace{Var[y]}_{noise} + \n",
    "\\underbrace{Var[\\hat{y}]}_{variance} + \n",
    "\\underbrace{\\{ y - \\mathbb{E}[\\hat{y}] \\}^2}_{bias^2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bayesian Linear Regression\n",
    "\n",
    "The likelihood of a linear regression problem is, \n",
    "\n",
    "> $L(X, w) = \\prod_{i=1}^N p(t_i|X_i, w)$\n",
    "\n",
    "where $p(t|x, w) = \\mathcal{N}(t|y(x, w), \\beta^{-1})$, \n",
    "\n",
    "thus the conjugate prior of the likelihood is also a Gaussian distribution.\n",
    "\n",
    "> $p(w) = \\mathcal{N}(w|\\mu_0, S_0)$\n",
    "\n",
    "The posterior distribution of $w$ is, \n",
    "\n",
    "> $p(w|t) = \\mathcal{N}(w|\\mu_1, S_1)$\n",
    "\n",
    "> $\\mu_1 = S_1(S_0^{-1}m_0 + \\beta \\phi^T t)$\n",
    "\n",
    "> $S_1^{-1} = S_0^{-1} + \\beta \\phi^T \\phi$\n",
    "\n",
    "The predictive result will be, \n",
    "\n",
    "> $p(t^{test}|t^{train}, w) = \\int p(t^{test}|x, w)p(w|t^{train})dw$\n",
    "\n",
    "Almost everything about Gaussian Distribution is analytical.\n",
    "\n",
    "> $p(t^{test}|t^{train}, w) = \\mathbb{E}_{w}[p(t^{test}|x, w)]$ \n",
    "\n",
    "To compute the expectation of the likihood function, if it is not analytical, we will use some approximation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sampling Methods\n",
    "\n",
    "## 3.0 Law of Large Number (Core)\n",
    "\n",
    "According to the law of large number, \n",
    "\n",
    "if $f(x_i)$ is drawn from the distribution of $p$.\n",
    "\n",
    "> $\\mathbb{E}_{p}[f(x)] = \\int f(x) p(x) dx \\thickapprox \\sum_{i=1}^N f(x_i)$\n",
    "\n",
    "## 3.1 Standard Distribution\n",
    "\n",
    "> <b>Theorem</b>: Assume we have a uniform random number generator $z \\sim U(0, 1)$, \n",
    "> $F^{-1}(z) \\sim F$.\n",
    "\n",
    "Proof:\n",
    "\n",
    "> $p(F^{-1}(z) \\le x)$\n",
    "\n",
    "> $= p(z \\le F(x))$, (Apply $F$ to both sides)\n",
    "\n",
    "> $= F(x)$, (Because $p(z \\le y) = \\int_{0}^{y}dz = y$)\n",
    "\n",
    "If we want to draw samples from $p(x)$ which is hard to drawn from, we can \n",
    "\n",
    "> + compute the cdf (cummulative distribution function $F(x)$;\n",
    "> + compute the inverse of cdf $F^{-1}(x)$;\n",
    "> + sample from the uniform distribution $z \\sim U(0, 1)$;\n",
    "> + obtain the sample value from $p(x)$, which is $F^{-1}(u)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: \n",
    "\n",
    "> $p(x=1) = 0.2, p(x=2) = 0.3, p(x=3) = 0.1, p(x=4) = 0.4$\n",
    "\n",
    "> $F(x=1) = p(x \\le 1) = 0.2, F(x=2) = p(x \\le 2) = 0.5, F(x=3) = p(x \\le 3) = 0.6, F(x=4)  = p(x \\le 4) = 1.0$\n",
    "\n",
    "Let $y = F^{-1}(x)$, $f(y) = x$,\n",
    "\n",
    "> $f(y=0.2) = 1, f(y=0.5) = 2, f(y=0.6) = 3, f(y=1.0) = 4$\n",
    "\n",
    "Draw $z \\sim U(0, 1)$, insert into $F^{-1}(z)$ we will obtain the samples drawn from $p(x)$,\n",
    "\n",
    "> $x = 1, if 0 < z \\le 0.2$\n",
    "\n",
    "> $x = 2, if 0.2 < z \\le 0.5$\n",
    "\n",
    "> $x = 3, if 0.5 < z \\le 0.6$\n",
    "\n",
    "> $x = 4, if 0.6 < z < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Rejection Methods\n",
    "\n",
    "Usually the cdf of $p(x)$ is hard to compute.\n",
    "\n",
    "Use a proposal distribution $q(x)$ which is easy to drawn from, and a constant $M$, \n",
    "\n",
    "> $Mq(x) \\ge p(x)$\n",
    "\n",
    "Sampling process\n",
    "\n",
    "+ Sample $x$ from $q(x)$\n",
    "\n",
    "+ Sample from the uniform distribution $u \\sim U(0, 1)$\n",
    "\n",
    "+ Check if $u \\le \\frac{p(x)}{Mq(x)}$, keep $x$, else reject the current sample.\n",
    "\n",
    "Sometimes, an unnormalized distribution $\\widetilde{p}(x)$ of $p(x)$ is used.\n",
    "\n",
    "## 3.3 Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
