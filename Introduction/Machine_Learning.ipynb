{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Machine Learning</h1>\n",
    "\n",
    "# 0. Logistic Regression\n",
    "\n",
    "Assume that the log odd is linear, \n",
    "> $\\log \\frac{p}{1-p} = \\sum_{i=1}^D w_i x_i$\n",
    "\n",
    "The positive probability is, \n",
    "\n",
    "> $p(y=1|W) = \\frac{1}{1+exp\\{-W^TX\\}}$\n",
    "\n",
    "Likelihood function is, let $p_i = p(y_i=1|W)$, \n",
    "\n",
    "> $L(X; \\theta) = \\prod_{i=1}^{N} p_i^{y_i} (1-p_i)^{1-y_i} $\n",
    "\n",
    "Log-likelihood function is, \n",
    "\n",
    "> $l(X; \\theta) = \\log L(X; \\theta) = \\sum_{i=1}^N y_i \\log p_i + (1 - y_i) \\log \\{1 - p_i \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. GBDT\n",
    "\n",
    "GBDT is an additional model, \n",
    "\n",
    "> $F_M(x) = \\sum_{m=1}^M T_m(x; \\theta_m)$\n",
    "\n",
    "> $F_M(x) = F_{M-1}(x) + T_M(x; \\theta_M)$\n",
    "\n",
    "The loss function for $(x_i, y_i)$ is, \n",
    "\n",
    "> $L(y_i, F_{M_i}(x)) = L(y_i, F_{M-1}(x_i) + T_M(x_i; \\theta_M))$\n",
    "\n",
    "> $= L(y_i, F_{M-1}(x_i)) + L' T_M(x_i; \\theta_M) + \\frac{1}{2} L'' T_M(x_i; \\theta_M)$\n",
    "\n",
    "> $L' = \\frac{\\partial{L(y_i, F_{M-1}(x_i))}}{\\partial{F_{M-1}(x_i)}}$\n",
    "\n",
    "If $L$ is squared loss, \n",
    "\n",
    "> $L(y_i, F_{M-1}(x_i)) = \\frac{1}{2}(y_i - F_{M-1}(x_i))^2$\n",
    "\n",
    "> $L' = F_{M-1}(x_i) - y_i = - (y_i - F_{M-1}(x_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to minimize the squared loss, according to the gradient descent method, \n",
    "\n",
    "> $y = y^{old} - \\alpha * \\nabla_{\\theta}$\n",
    "\n",
    "> $y_M(x) = F_M(x) = y_{M-1}(x) - L'$\n",
    "\n",
    "> $= y_{M-1}(x) + (y_i - F_{M-1}(x_i))$\n",
    "\n",
    "> $= y_{M-1}(x) + T_M(x_i; \\theta)$\n",
    "\n",
    "That is, \n",
    "\n",
    "> $T_M(x_i; \\theta) = y_i - F_{M-1}(x_i)$\n",
    "\n",
    "the $M$-th tree tries to fit the residual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SVM\n",
    "\n",
    "Assume a hyperline $W^T X + b = 0$ can classify the datasets, \n",
    "suppose the point with least distance to the seperate line lies on $W^T X + b = +1$, (or -1)\n",
    "\n",
    "we want to maximize the distance between these two lines, \n",
    "\n",
    "> $\\max_W \\frac{2}{||W||^2}$\n",
    "\n",
    "> $s.t. y_i(W^T X_i + b) \\ge 1$\n",
    "\n",
    "Equivalent to, \n",
    "\n",
    "> $\\min_{W} \\frac{1}{2} W^2$\n",
    "\n",
    "> $s.t. y_i(W^T X_i + b) \\ge 1$\n",
    "\n",
    "The optimization of unequal formula satisfy the K.K.T conditions, \n",
    "\n",
    "define the Lagrange problem, \n",
    "\n",
    "> $L(W, b, \\alpha) = \\frac{1}{2} W^2 - \\sum_{i=1}^N \\alpha_i * (y_i(W^T X_i + b) - 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primal Problem\n",
    "\n",
    "The primal problem of the Lagrange problem is, \n",
    "\n",
    "> $\\min_{W, b}\\max_{\\alpha \\ge 0} L(W, b, \\alpha)$\n",
    "\n",
    "## Dual Problem\n",
    "\n",
    "The dual problem is, \n",
    "\n",
    "> $g = \\max_{\\alpha \\ge 0} \\min_{W, b} L(W, b, \\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reinforcement Learning\n",
    "\n",
    "Reinforcement learning is formulated as Markov Decsion Process.\n",
    "\n",
    "To obtain a maximum expected, accumulated, disaccounted, future award.\n",
    "\n",
    "Given 5-tuple $(S, A, R, T, \\gamma)$, which indicates state, action, reward, transition among states and reward discount rate separately.\n",
    "\n",
    "## 3.0 Objective\n",
    "\n",
    "Find optimal policy $\\pi^*$ to get the maximize accumlative reward, \n",
    "\n",
    "> $\\sum_{t=0}^{\\infty} \\gamma^t r_t$\n",
    "\n",
    "## 3.1 Value function\n",
    "\n",
    "> $V(s_t) = \\mathbb{E}_{\\pi} [\\sum_{i=0}^{\\infty} \\gamma^i r_{t+i}|s_t = s]$\n",
    "\n",
    "The Bellman equation will get a recursive formulation of value function, thus we can compute the function iteratively.\n",
    "\n",
    "> $V(s_t) = \\mathbb{E}_{\\pi} [r_t + \\gamma \\mathbb{E}_{\\pi}[V(s_{t+1})]   |s_t = s]$\n",
    "\n",
    "## 3.2 Q function\n",
    "\n",
    "> $Q(s, a) = \\mathbb{E}_{\\pi} [ \\sum_{t=0}^{\\infty} \\gamma^t r_t | s_0 = s, a_0 = a ]$\n",
    "\n",
    "> $Q^{*}(s, a) = \\max \\mathbb{E}_{\\pi} [ \\sum_{t=0}^{\\infty} \\gamma^t r_t | s_0 = s, a_0 = a ]$\n",
    "\n",
    "> $= \\mathbb{E}_{\\pi, s' \\in \\mathcal{S}} [r + \\gamma \\max_{a'} Q(s', a') |s, a ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Methods\n",
    "\n",
    "### 3.3.0 Dynamic Programming\n",
    "\n",
    "Boostrapping method, update the function iteratively according to the Bellman Equation.\n",
    "\n",
    "### 3.3.1 Time Difference\n",
    "\n",
    "Somehow likely to the gradient descent method, update the function according to the difference between past and current situation.\n",
    "\n",
    "#### Sarsa\n",
    "\n",
    "An on-policy method, \n",
    "\n",
    "> $Q(s, a) = Q(s, a) + \\alpha [r + Q(s', a') - Q(s, a)]$\n",
    "\n",
    "### 3.3.2 Monte Carlo\n",
    "\n",
    "Obtain a series of samples according to policy, then compute the expected, accumulated, discounted rewards and update the funtion.\n",
    "\n",
    "### 3.3.3 Q-learning\n",
    "\n",
    "> $Q^*(s, a) = \\mathbb{E}[r + \\max_{a'} Q^*(s', a')|s, a]$\n",
    "\n",
    "Using function to approximate the q function.\n",
    "\n",
    "> $Q(s, a; \\theta) \\approx Q^*(s, a)$\n",
    "\n",
    "+ Loss function\n",
    "\n",
    "> $L_i(\\theta_i) = \\mathbb{E}_{s, a}[(y_i - Q(s, a; \\theta_i))^2]$\n",
    "\n",
    "> $y_i = \\mathbb{E}_{s'}[r + \\gamma \\max_{a'} Q(s', a'; \\theta_{i-1})]$\n",
    "\n",
    "+ Gradient\n",
    "\n",
    "> $\\nabla_{\\theta_i}(L_i(\\theta_i)) = \\mathbb{E}_{s, a, s'}[(Q(s, a; \\theta_i) - r - \\gamma \\max_{a'} Q(s', a'; \\theta_{i-1})) \\nabla_{\\theta_i} Q(s, a; \\theta_i)] $\n",
    "\n",
    "### 3.3.4 DQN\n",
    "\n",
    "Use deep neural network as the approximation function.\n",
    "\n",
    "+ Experience Replay.\n",
    "\n",
    "Use memory to store the situation $(s_t, a_t, r_t, s_{t+1})$\n",
    "\n",
    "+ Minibatch sampling.\n",
    "\n",
    "Disadvatage: \n",
    "\n",
    "> + Q-function will be complicated\n",
    "\n",
    "> + Action (discrete) might be high dimensional.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 DDPG\n",
    "\n",
    "An Actor-Critic method, \n",
    "\n",
    "Actor (policy gradient)\n",
    "\n",
    "Critic (Q-learning)\n",
    "\n",
    "+ Initialize Critic Network $Q(s, a | \\theta^Q)$,\n",
    "actor network $\\mu(s|\\theta^\\mu)$\n",
    "\n",
    "+ Initialize target network $Q'$ and $\\mu'$ with the same weights\n",
    "\n",
    "+ Update critic network, according to the loss, \n",
    "\n",
    "> $L = \\frac{1}{N} \\sum_{i}(y_i - Q(s_i, a_i|\\theta^Q))^2$\n",
    "\n",
    "+ Update actor network, using the sampled policy gradient, \n",
    "like the chain-rule.\n",
    "\n",
    "> $\\frac{\\partial{J}}{\\partial{\\theta}} = \n",
    "\\frac{\\partial{J}}{\\partial{a}}\n",
    "\\frac{\\partial{\\mu}}{\\partial{\\theta^{\\mu}}}$\n",
    "\n",
    "> $\\nabla_{\\theta^\\mu} J \\approx \\frac{1}{N}\n",
    "\\nabla_a Q(s, a|\\theta^Q)|s=s_i, a=\\mu(s_i) \n",
    "\\nabla_{\\theta^\\mu} \\mu(s|\\theta^\\mu)|s_i$\n",
    "\n",
    "+ Update the target network, delay updated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
